\section{Gesamtkonzept}

In den folgenden Kapiteln wird das erarbeitete Konzept des Roboters vorgestellt. Dafür wird zuerst eine Skizze gezeigt, die den geplanten Roboter visualisiert, danach werden die Komponenten und Technologien, die im Roboter verwendet werden, aufgelistet. Nachfolgend werden die einzelnen Teilfunktionen, die der Roboter während einem Durchlauf benötigt erläutert. 

Das Konzept wurde mithilfe des Prototypings getestet. Dies ist im Anhang in Kapitel \nameref{prototyping} zu finden.

\subsection{Visualisierung}

Die Einzelteile des Roboters sollen wie auf Abbildung \ref{fig:robot_concept-scetch_labeld} gezeigt zusammengesetzt werden.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{assets/gesamtkonzept/Skizze-Fahrzeugkonzept-Beschriftet.jpg}
\caption{Konzeptskizze Gesamtkonzept}
\label{fig:robot_concept-scetch_labeld}
\end{figure}

\subsection{Komponenten}
Für das Konzept wurden die auf Abbildung \ref{table:mk-all} ersichtlichen Komponenten mithilfe der Technologierecherche (siehe Anhang \nameref{techrecherche}) und anschliessenden morphologische Kästen (siehe Anhang \nameref{mk}) und Nutzwertanalysen (siehe Anhang \nameref{nutzwertanalyse}) ermittelt. 

\begin{table}[H]
\centering
\includegraphics[width=\textwidth -20mm]{assets/MK-all.pdf}
\caption{Morphologischer Kasten: Gesamtkonzept}
\label{table:mk-all}
\end{table}

Es ist geplant einen Roboter in Form eines freien Polygons zu bauen, der sich mit drei Rädern fortbewegt und eine Motorlenkung besitzt. Hindernisse kann der Roboter dank einem Greifer mit flexiblen Backen anheben. Dies ist in der vorherigen Abbildung \ref{fig:robot_concept-scetch_labeld} ersichtlich .

Die Steuerung wird auf einem \gls{tinyk22} betrieben. Die Distanz zu den Objekten wird mittels einem Ultraschallsensor erkannt. Die bürstenbehafteten Motoren werden mit einem gekauften Treiber angesteuert. Die Stromversorgung läuft über einen Akku. Der Akkustand und der Status des Roboters werden durch \acrfull{led} angezeigt. Damit der Roboter die Linien erkennt, wird ein Liniensensor mit Fototransistoren verwendet. Das Ziel wird über einen Drehschalter vom Benutzer ausgewählt.
Wenn der Roboter das Ziel erreicht, verkündet er dies über einen Lautsprecher. Im Notfall wird der Roboter über einen Taster ausgeschaltet.

Die Bildverarbeitung und Navigation werden in Python programmiert und laufen auf einem Raspberry Pi. Zur Bilderkennung wird eine Kombination von \gls{pytorch} und \gls{opencv} verwendet. Die Bilder werden mit einer Raspberry Pi Camera aufgenommen. Der kürzeste Weg zum Ziel wird mit einem \gls{dijkstra} Algorithmus berechnet.

\subsection{Ablauf}

Der Ablauf einer Durchfahrt ist im Ablaufdiagramm \ref{fig:ablaufdiagramm} aufgezeigt.
Die Schritte, die mit einem Plus markiert sind, sind in den folgenden Kapiteln als Subprozesse detailliert definiert.

Um den Graph zu traversieren, wird eine iterative Herangehensweise umgesetzt. Durch die Spiegelung des Bodens, die Grösse des Graphes und die Einschränkung der Höhe des Roboters, ist es nicht möglich, am Start ein Bild des gesamten Graphens zu machen und dabei die Hindernisse und die fehlenden Linien zu erkennen. Deshalb werden iterativ bei jedem Knoten die ausgehenden Linien geprüft.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{assets/gesamtkonzept/ablaufdiagramm.png}
\caption{Ablaufdiagramm}
\label{fig:ablaufdiagramm}
\end{figure}

\subsubsection{Ausgehende Kanten erkennen}\label{outgoing-angles}

Um alle ausgehenden Kanten und deren Winkel zu erkennen, wird der folgende Ablauf in Abbildung \ref{fig:ablaufdiagramm-kanten-erkennen} durchlaufen. Dies ist nötig, damit der Roboter weiss, in welche Richtung er sich drehen muss, um auf die nächste Kante zu gelangen. Ebenfalls können so fehlende Linien erkannt werden.

Falls es weniger Kanten gibt als erwartet, werden die erhaltenen Winkel zu den einzelnen möglichen Bereichen zugeordnet. In dem Bereich, in welchem kein Winkel zugeordnet wurde, fehlt eine Linie. Folglich aktualisiert der Roboter seine internen Informationen: Eine Linie wird aus dem Grundgraphen entfernt und die Basiswerte der Winkel werden mit den gemessenen Werten ersetzt.

Nachdem der Roboter den nächsten Knoten berechnet hat, wird der Winkel zur richtigen ausgehenden Kante an die Steuerung gesendet. Der Roboter dreht sich, um auf dieser Linie weiterzufahren.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{assets/gesamtkonzept/ablaufdiagramm-kanten-erkennen.png}
\caption{Ablaufdiagramm ausgehende Kanten erkennen}
\label{fig:ablaufdiagramm-kanten-erkennen}
\end{figure}

\textbf{Ablauf detailliert erklärt}

Der Roboter bewegt sich auf einen Knoten zu und hält 15 cm vor diesem an. Der Knoten wird fotografiert und der Roboter fährt anschliessend auf den Knoten. Das Bild wird mit \gls{opencv} zuerst so entzerrt, dass der Knoten gerade von oben dargestellt wird. Danach werden die einzelnen Winkel gemessen. Dies ist im Prototyping Kapitel im Anhang \nameref{winkelerkennung} ausführlicher beschrieben. Das Resultat dieser Objekterkennung ist eine Liste mit Winkeln. Im Fall, der auf Abbildung \ref{fig:angle-recognition} gezeigt ist, würde die Liste \verb|[3, 41, 68, 127]| erstellt werden.

\begin{figure}[H]
\centering
\begin{subfigure}{0.45\textwidth}
\centering
\includegraphics[width=0.95\linewidth]{assets/informatik-prototyp/opencv/angle_detection/image_taken_by_pi_camer_before_node.jpg} 
\caption{Knoten mit 15cm Abstand}
\label{fig:node-15cm-before}
\end{subfigure}
\begin{subfigure}{0.45\textwidth}
\centering
\includegraphics[width=0.95\linewidth]{assets/informatik-prototyp/opencv/angle_detection/node_with_edge_angles_annotated.png} 
\caption{Knoten mit gemessenen Winkeln}
\label{fig:node-angles}
\end{subfigure}

\caption{Winkelerkennung}
\label{fig:angle-recognition}
\end{figure}

Die erhaltene Liste mit Winkeln wird nun verwendet, um mögliche fehlende Linien zu erkennen und die Winkel intern zu speichern. Die internen Winkel werden verwendet, um den Roboter richtig auszurichten, wenn dieser auf die nächste Linie fährt.

Der Roboter selber hat einen Grundgraphen mit Winkeln gespeichert. Diese Winkel ergeben sich aus dem zur Verfügung gestellten Graph. Dabei werden die Winkel der Kanten im Graphen im Uhrzeigersinn gelesen. Dabei ist eine Linie, die nach 12 Uhr ausgeht, 0\textdegree, eine Linie nach 3 Uhr ist 90\textdegree, 6 Uhr ist 180\textdegree und 9 Uhr ist 270\textdegree. Der Roboter merkt sich, in welche Richtung er gerade schaut. Damit ergibt sich ein Offset, der verwendet wird, um die Winkel, die wie eine Uhr gelesen wurden, zu übersetzen. Mit dieser Information kann sich der Roboter dann richtig ausrichten.

Die Abbildung \ref{fig:angle-graphs-internal} soll visualisieren, wie die Winkel von jedem Knoten intern gespeichert werden. Auf jedem Knoten befindet sich eine Uhr, die aufzeigen soll, wie die Winkel mit den einzelnen Uhrzeiten korrespondieren. Intern wird 12Uhr (oben) immer als 0\textdegree\ festgehalten. Das ist der Ausgangspunkt für die Berechnungen von Drehwinkeln.

Angenommen der Roboter fährt von links auf den Knoten G zu, dann weiss er durch die intern gespeicherte Ausrichtung, dass er von 270\textdegree\ herkommt. Wenn er nun den Knoten mit den ausgehenden Kanten fotografiert, erhält er die Liste \verb|[0, 120, 180, 240, 300]|.  Jeder Wert in der Liste wird mit dem Herkunftswinkel des Roboters addiert. Diese Resultate werden modulo 360 gerechnet und es ergibt sich eine Winkelliste \verb|[30, 90, 150, 210, 270]| im Format einer Uhr. Mit dieser kann nun die Zuordnung der Kanten mit den intern gespeicherten Winkeln durchgeführt werden.

Falls der Roboter sich nun in Richtung zum Knoten E ausrichten will, wird der gewünschte Winkel minus dem Herkunftswinkel der Roboters + 180\textdegree\ und gerechnet und das Resultat modulo 360:

\[Drehwinkel = (GewuenschterAusgangswinkel - Herkunftswinkel + 180^\circ) \mod 360\]

Mit obigen Beispiel:
\[Drehwinkel = (30^\circ - 270^\circ + 180^\circ) \mod 360 = -60^\circ\]

Wenn der Drehwinkel >180 ist, kann nochmals minus 360\textdegree\ gerechnet werden, damit gegen den Uhrzeigersinn gedreht wird und so der Weg kürzer ist.


\begin{figure}[H]
\centering
\begin{subfigure}{0.98\textwidth}
\centering
\includegraphics[width=0.95\linewidth]{assets/informatik-prototyp/simulator/internal-angles.png} 
\caption{Winkel im Graph}
\label{fig:labeled-graph-angles}
\end{subfigure}
\begin{subfigure}{0.8\textwidth}
\begin{footnotesize}
\begin{verbatim}

angles = {
    [...],
    'G': {'E': 30, 'D': 90, 'H': 150, 'A': 210, 'F': 270},
    [...],
    }
\end{verbatim}
\end{footnotesize}
\caption{Graph mit Winkeln in YAML Knoten G}
\label{fig:graph-yaml-angle}
\end{subfigure}
\caption{Graph mit Winkeln}
\label{fig:angle-graphs-internal}
\end{figure}

Da der Graph nicht genau so aufgeklebt sein wird wie auf der Skizze, wurden zusätzliche Bereiche definiert, in denen sich die Winkel befinden sollten.

Ein Ausschnitt, der zeigt, wie der interne Graph in einem \gls{yaml} File definiert ist, ist hier eingefügt.

\begin{verbatim}
C: [{D: [0, [30, 120]}, {B: [240, [120, 30]}, {H: 300, [30, 30]}],
\end{verbatim}

Auf Grafik \ref{fig:angled-graph} sind alle Winkel eingezeichnet inklusive der Halbwinkel zwischen allen Kanten. In Grafik \ref{fig:excerpt-angled-graph} ist die Kante zwischen C und D gezeigt, dies entspricht \verb|'C: [{D: [0, [30, 120]}'| im \gls{yaml} File. Da die Linie von C nach D gerade nach oben (nach 12 Uhr) weggeht, ist dies im Idealfall 0\textdegree.
Das der Idealfall nicht eintreten wird, da der Graph nicht perfekt aufgeklebt sein wird und die Linie nicht perfekt in Richtung 12 Uhr gehen wird, sind die Halbwinkel eingezeichnet, hier 30\textdegree\ und 120\textdegree. Die Kante zu Knoten D kann also im Bereich von 0\textdegree-30\textdegree\ und 0\textdegree+120\textdegree, sprich -30\textdegree\ und 180\textdegree\ liegen. Befindet sich eine gemessene Kante in diesem Bereich, wird es die Kante C zu D sein.

\begin{figure}[H]
\centering
\begin{subfigure}{0.65\textwidth}
\centering
\includegraphics[width=0.95\linewidth]{assets/informatik-prototyp/graph-angles.png} 
\caption{Graph mit Winkeln und Winkelbereichen}
\label{fig:angled-graph}
\end{subfigure}
\begin{subfigure}{0.32\textwidth}
\centering
\includegraphics[width=0.95\linewidth]{assets/informatik-prototyp/c-d-angle-labeled.png} 
\caption{C zu D Winkel}
\label{fig:excerpt-angled-graph}
\end{subfigure}
\caption{Winkel und Halbwinkel im Graphen}
\label{fig:angles}
\end{figure}


\subsubsection{Zielknoten erkennen}

Damit der Roboter überprüfen kann, ob er wirklich das Ziel erreicht hat, muss erkannt werden können, ob ein Knoten der richtige Zielknoten ist. Dazu wird der \acrfull{orb} Algorithmus verwendet, der Teil der \gls{opencv} Bibliothek ist.\gls{orb-gloss} lernt die Merkmale von Objekten und erkennt auf einem Bild, welches Objekt gefunden wurde.

Der Algorithmus wird so eingesetzt werden, dass der Roboter bevor er den Lauf beginnt die Merkmal der drei Buchstaben lernt. Jedes Mal, wenn ein Knoten fotografiert wird, um die ausgehenden Kanten zu prüfen, wird ebenfalls geprüft, ob sich ein Buchstaben darauf befindet. Falls ein Buchstaben detektiert wird, bestimmt \acrshort{orb} um welchen es sich handelt.

Dadurch dass \acrshort{orb} die Merkmale der Buchstaben detektiert, ist es nicht nötig, die Rotation der Buchstaben zu beachten. Unabhängig von welcher Richtung der Roboter den Zielknoten fotografiert, kann der Buchstabe erkannt werden.

Auf der folgenden Grafik \ref{fig:orb-zielknoten-konzept} wird mit den farbigen Linien dargestellt, wie \acrshort{orb} die bekannten Merkmale (jeweils linkes Bild) in den zu analysierenden Bildern (jeweils rechtes Bild) findet.

\begin{figure}[H]
\centering
\begin{subfigure}{0.3\textwidth}
\centering
\includegraphics[width=0.95\linewidth]{assets/informatik-prototyp/opencv/target_node_detection/orb-a.png} 
\caption{ORB A}
\label{fig:orb-a-konzept}
\end{subfigure}
\begin{subfigure}{0.3\textwidth}
\centering
\includegraphics[width=0.95\linewidth]{assets/informatik-prototyp/opencv/target_node_detection/orb-b.png} 
\caption{ORB B}
\label{fig:orb-b-konzept}
\end{subfigure}
\begin{subfigure}{0.3\textwidth}
\centering
\includegraphics[width=0.95\linewidth]{assets/informatik-prototyp/opencv/target_node_detection/orb-c.png} 
\caption{ORB C}
\label{fig:orb-c-konzept}
\end{subfigure}

\caption{Zielknotenerkennung mit ORB}
\label{fig:orb-zielknoten-konzept}
\end{figure}


Mit \acrshort{orb} werden nur die Buchstaben A, B oder C erkannt, da der trainierte Algorithmus nur diese drei Zeichen kennt. So kann ausgeschlossen werden, dass ein C beispielsweise als das Klammer Zeichen \verb|(| erkannt werden würde.


\subsubsection{Pylonen und Hindernisse erkennen}

Damit der Roboter weiss, ob sich eine Pylone auf einem Nachbarsknoten oder eine Barriere auf einer ausgehenden Linie befindet, wird der Ablauf auf Abbildung \ref{fig:image-detection-obstacles} durchgeführt.

\begin{figure}[H]
\centering
\begin{subfigure}{0.45\textwidth}
\centering
\includegraphics[width=0.95\textwidth]{assets/gesamtkonzept/ablaufdiagramm-hindernisse-erkennen.png}
\caption{Ablaufdiagramm Hindernis erkennen}
\label{fig:ablaufdiagramm-hindernis-erkennen}
\end{subfigure}
\begin{subfigure}{0.5\textwidth}
\centering
\includegraphics[width=0.95\textwidth]{assets/informatik-prototyp/yolo/recognized-images.jpeg}
\caption{YOLOv11 Bilderkennung}
\label{fig:img-recognition-yolo-concept}
\end{subfigure}
\caption{Bilderkennung Hindernisse}
\label{fig:image-detection-obstacles}
\end{figure}

Aus dem vorherigen Schritt des Winkelerkennens, kennt der Roboter alle ausgehenden Linien und deren Position. Er dreht sich nun im Uhrzeigersinn zu jeder Kante und fotografiert diese. Durch das Hochformat der Kameras sieht er weit in die Ferne und kann auch nur die wichtigen Elemente sehen, sprich, diese, die sich auf der Linie befinden.

Die Bilder werden mit dem \gls{yolo} Objekterkennungsalgorithmus ausgewertet. Dabei werden sowohl Knoten, als auch Pylonen und Barrieren erkannt. Das erhaltene Resultat beschreibt anhand der definierten Klassen, welche Elemente erkannt wurden und mithilfe der Koordinaten, wo sich diese auf dem Bild befinden.
Alle Hindernisse werden intern gespeichert indem die jeweiligen Kanten höher gewichtet oder Knoten entfernt werden.


\newpage

\subsubsection{Hindernisse bewegen}
\label{subsubsection:Hindernisse bewegen}

Um ein vorhandenes Hindernis zu bewegen, wird ein Mechanismus zum Anheben, nachfolgend Greifer genannt, verwendet. Nach dem Anheben bewegt sich der Roboter und setzt das Hindernis an der alten Position ab.
Mit nur einem Motor wird das Hindernis eingeklemmt, zentriert und angehoben. Dieser Vorgang und der Aufbau des Greifers wird in folgendem Kapitel erläutert.

Die gezeigten Abbildungen des Greifers stammen von einem Prototyp und stellen nicht das finale Design dar. Sie dienen lediglich zur Veranschaulichung der Funktionsweise. Die Dimensionen des Gestänges sowie die Positionen der Lagerstellen sollen für die finale Variante beibehalten werden.

In der Nutzwertanalyse (Anhang \nameref{nutzwertanalyse}) hat man sich zum Anheben des Hindernisses für ein Klemm-Design entschieden, welcher das Hindernis oben an der längsten Kante an 3 Punkten einspannt (siehe Abb. \ref{fig:obstacle_clamping_concept}). 


\begin{figure}[H]
\centering
\includegraphics[width=0.95\linewidth]{assets/greifer-prototyp/Greifer_Backen_Trimetric.png} 
\caption{Position Klemmbacken}
\label{fig:obstacle_clamping_concept}
\end{figure}

\newpage

Um sowohl das Klemmen, als auch das Anheben des Hindernisses mit einem einzelnen Servomotor zu realisieren, wurde der Mechanismus als ein Gestänge realisiert (Abb. \ref{fig:gripper_components}).

\begin{figure}[H]
\centering
\includegraphics[width=1.0\linewidth]{assets/greifer-prototyp/Greifer_side_Komponentennamen.png} 
\caption{Komponenten des Greifers}
\label{fig:gripper_components}
\end{figure}

Die Greifbacke ist gelenkig mit dem Greifarm verbunden, welcher von der Pendelstütze geführt wird. Am Ende des Greifarms wird der Arm des Servomotors befestigt. Der Servomotor ist in der Abbildung durch einen Hebel ersetzt, da der Prototyp zu Beginn des Prototypings von Hand bedient wird. Die zwei Spannbacken sind am Spannarm befestigt. Dieser Spannarm ist drehbar am Chassis des Fahrzeugs gelagert und wird durch eine Zugfeder vorgespannt. Abgebildet sind nur die Montagepunkte der Feder.

\newpage

Dreht der Servomotor im Uhrzeigersinn, so schwingen Greifarm und Greifbacke nach oben weg. Der Greifer ist geöffnet (Abb. \ref{fig:gripper_opening_side}). So kann der Roboter auf das Hindernis zu fahren, bis dieses die zwei Spannbacken berührt. An beiden Spannbacken wird jeweils ein Endschalter montiert, welcher erkennt, wann das Hindernis nahe genug ist und anschliessend den Hebeprozess startet. Dieser Endschalter ist auf der Abbildung \ref{fig:gripper_opening_side} nicht ersichtlich.

Dreht der Servomotor aus dieser Position im Gegenuhrzeigersinn, so schwingt die Greifbacke zurück, bis sie in Berührung mit dem Hindernis kommt. Dadurch wird das Hindernis gegen die zwei Spannbacken gedrückt, welche wiederum durch die Vorspannung der Feder gegen das Hindernis drücken. Das Hindernis wird eingeklemmt (Abb. \ref{fig:gripper_gripping_side}).

Dreht der Servomotor weiter im Gegenuhrzeigersinn, so dreht nun auch der Spannarm im Gegenuhrzeigersinn mit, da er über das Hindernis vom Greifarm bewegt wird. Die Rotation des Spannarms führt dazu, dass das Hindernis angehoben wird (Abb. \ref{fig:gripper_lifting_side}). Gleichzeitig wird durch die Rotation des Spannarms die Zugfeder leicht verlängert. Somit wird die Klemmkraft auf das Hindernis erhöht und sichergestellt, dass dieses nicht aus dem Greifer rutscht.

\begin{figure}[H]
\centering
\begin{subfigure}{0.49\textwidth}
\centering
\includegraphics[width=\textwidth]{assets/greifer-prototyp/Greifer_side_Offen.png}
\caption{öffnen}
\label{fig:gripper_opening_side}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
\centering
\includegraphics[width=\textwidth]{assets/greifer-prototyp/Greifer_side_Klemmen.png}
\caption{klemmen}
\label{fig:gripper_gripping_side}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
\centering
\includegraphics[width=\textwidth]{assets/greifer-prototyp/Greifer_side_Angehoben.png}
\caption{anheben}
\label{fig:gripper_lifting_side}
\end{subfigure}
\caption{Ablauf Hindernis anheben}
\label{fig:obstacle_gripping_process}
\end{figure}

 \newpage
 
Als Basis zur Auslegung des Greifers dient eine Berechnung der nötigen Klemmkraft, um das Hindernis anzuheben. Dazu wurden ein Haftreibwert von 0.3 zwischen Hindernis und Klemmbacken und eine Sicherheit gegen das Rutschen von 1.5 angenommen. Mit der Klemmkraft konnte anhand der Geometrie des Greifers eine Feder mit ausreichend hoher Federkonstante ausgewählt werden. Schliesslich wurde zur Auswahl des Servomotors das nötige Drehmoment berechnet, um sowohl die Zugfeder zu verlängern, als auch das Hindernis anzuheben. Die detaillierten Berechnungen sind im Kapitel \nameref{subsubsection:gripper-calculations} zu finden. Die Ergebnisse aus den Berechnungen wurden im Kapitel \nameref{subsubsection:gripper-prototype-1} anhand eines Prototyps validiert.

Das Klemmen und Anheben ist nur ein Teilschritt  zur Beseitigung eines Hindernisses. Nachfolgend wird der gesamte Ablauf erläutert. Ausgangslage bildet der Ultraschallsensor, der das Hindernis erkannt hat und den Prozess einleitet. 

\begin{figure}[H]
\centering
\includegraphics[width=0.2\textwidth]{assets/gesamtkonzept/ablaufdiagramm-hindernis-bewegen.png}
\caption{Ablaufdiagramm Hindernis bewegen}
\label{fig:ablaufdiagramm-hindernis-bewegen}
\end{figure}

 Mit einem Ultraschall-Sensor soll das Vorhandensein eines Hindernisses und die ungefähre Distanz dazu bestimmt werden. Zur genauen Bestimmung der Distanz vor dem Greifer wird der Endschalter am Greifmechanismus verwendet.
Greifer und Endschalter werden sich an der Rückseite des Fahrzeugs befinden, der Ultraschallsensor vorne (siehe Abb. \ref{fig:robot_concept-scetch_labeld}). Dadurch muss sich das Fahrzeug, nachdem ein Hindernis mittels Ultraschall entdeckt wurde, 180\textdegree\ drehen und langsam rückwärts fahren, bis der Endschalter am Greifer betätigt wird, um das Hindernis anzuheben. Sobald das Hindernis angehoben ist, dreht sich das Fahrzeug 180\textdegree\ und fährt 30mm vorwärts, um das Hindernis an dieselbe Stelle zurückzusetzen. Das Fahrzeug steht nach dem Absetzen wieder nach vorne ausgerichtet und kann geradeaus weiter fahren (siehe  Abb. \ref{fig:ablaufdiagramm-hindernis-bewegen}).


\subsection{Fortbewegung/ Fahrwerk}

Das Fahrwerk wird nach Abb.  \ref{fig:Prototype_Fahrwerk_CAD}  Prototyp Fahrwerk aufgebaut. Das Fahrzeug wird mit zwei einzeln angesteuerten Räder angetrieben. Damit wird sowohl das Vorwärtsfahren, als auch das Drehen der Räder durchgeführt. Zusätzlich zu den beiden angetriebenen Rädern besitzt das Fahrzeug ein frei-drehendes Abstützrad. Direkt vor dem Abstützrad wird der Distanzsensor montiert. ( \ref{subsection:Distanzsensor} Distanzsensor) Die \acrfull{dc} Motoren besitzen Encoder und sind über eine Welle direkt mit den Antriebsrädern verbunden. Dadurch kann genau festgestellt werden, welche Distanz zurückgelegt wurde. Der Fahrbefehl entsteht aufgrund der Berechnungen der Bilderkennung. Dem Mikrocontroller \gls{tinyk22}, der die Motoren ansteuert, werden Distanz und Drehwinkel mitgeteilt. 

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{assets/prototyp-fahrwerk/Prototyp_Fahrwerk_CAD.png}
\caption{Prototyp Fahrwerk}
\label{fig:Prototype_Fahrwerk_CAD}
\end{figure}


\subsection{Linienerkennung}

Mit einem Array von Phototransistoren, Kondensatoren und Widerständen, wird die Entladezeit von Kondensatoren mittels Mikrocontroller und der Input Capture Funktion gemessen. Der Liniensensor dient als Unterstützung, damit der Roboter die Linie nicht verlässt während dem Fahren. Zusätzlich soll der Roboter bevor er losfährt, gerade vor der Linie positioniert werden, so dass er sich zumindest zu Beginn der Fahrt bereits auf der Linie befindet. Ebenfalls wird der Liniensensor gebraucht, um zu prüfen ob der Roboter auf einem Knoten steht.

\subsection{Distanzsensor}
\label{subsection:Distanzsensor}

Um die Distanz zwischen dem Roboter und einem Hindernis zu detektieren wird ein Ultraschallsensor verwendet. Der Sensor wird wie auf Abb \ref{fig:Prototype_Sensorhalter} Prototyp Sensorhalter oberhalb des Abstützrades montiert. Der Sensor erkennt die Distanz zum Hindernis und kann bestimmen, an welchem Punkt das Fahrzeug eine 180-Grad-Drehung durchführen muss, um sich auf den Hubvorgang vorzubereiten. Ebenfalls wird so ein Zusammenstossen mit Hindernissen verhindert, falls diese bei der Bilderkennung zuvor nicht erkannt wurden.

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{assets/prototyp-fahrwerk/Prototyp_Sensorhalter.png}
\caption{Prototype Sensorhalter}
\label{fig:Prototype_Sensorhalter}
\end{figure}



\subsection{Wegfindung}

Der kürzeste Weg im Graphen vom momentanen Knoten zum Zielknoten, wird mit dem \gls{dijkstra} Algorithmus berechnet. Dieser wird zu Beginn berechnet und jedes Mal, wenn der Roboter neue Erkenntnisse zum Graph gesammelt hat, aktualisiert. 

Zusätzlich zum zukünftigen Pfad, wird der bereits befahrene Pfad gespeichert. Dies dient dazu, dass der Roboter immer in der Lage sein wird, im Fehlerzustand, auf den letzten Knoten zurück zu fahren und dabei die Orientierung behält.

\subsection{Kameraposition}

Die Kamera wird in einer Höhe von ca. 22.5cm und einem Winkel von ca. 56\textdegree\ montiert. Die Position ist fix, das heisst, es wird keine schwenkbare Kamera benötigt. In der folgenden Grafik \ref{fig:camera-position-concept} ist die Kameraposition ersichtlich. Die Kamera selbst verfügt über ein horizontales Field of View\footnote{\url{https://en.wikipedia.org/wiki/Field_of_view}} von 66\textdegree. Die Kamera wird im Hochformat verwendet. Dadurch sind im Field of View von 66\textdegree\ sowohl sehr nahe Knoten und Objekte bis zu 10cm im Bild, aber auch weit entfernte Pylonen, welche 200cm entfernt stehen.

Sowohl die Montage, als auch das Fiel of View ist in Abbildung \ref{fig:camera-position-concept} ersichtlich.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{assets//informatik-prototyp//camera/camera_position.png}
    \caption{Kamera Positionierung}
    \label{fig:camera-position-concept}
\end{figure}



\subsection{Software Steuerung}


Das Programm, welches auf das \gls{tinyk22} geladen wird, wird in C geschrieben. Mit der Entwicklungsumgebung MCUXpressoIDE kann der Code geschrieben und kompiliert werden. Das Programm kann debugged werden, da auf dem Tiny K22 ein Debugger beinhaltet ist. Es können vom Modul ``Mikrocontroller Fundamentals'' bestehende Libraries verwendet werden. Einige Anpassungen, wie zum Beispiel die \acrshort{uart}, wurden bereits vorgenommen.

\subsection{Akku}

Um das ganze System mit Spannung zu versorgen wird ein 14.8V Akku verwendet, der in einer Stunde 3000mA liefert. Daraus kann berechnet werden, dass bei einer geschätzten maximalen Leistung von 30W und einer Spannung von 12V, diese Kapazität für ca. 1.2 Stunden reicht.  

\subsection{PCB Design}

Das \acrshort{pcb} wird aus mehreren Teilen bestehen. Das Mainboard wird zentral eingebaut, da es der Knotenpunkt und der grösste \acrshort{pcb} Komponent ist. Ebenfalls wird eine separate Platine für die Spannungsversorgung benötigt, welche sich ebenfalls zentral befinden wird. Der Ultraschall- und Liniensensor befinden sich im vorderen Teil des Roboters und werden über eine Kabelverbindung mit dem Mainboard angeschlossen. Aufgrund der Wiederverwendung werden die einzelnen Komponenten, wie auch das \gls{tinyk22}, steckbar verbunden werden.

\newpage

\subsection{Schnittstellen zwischen den Kompontenten}

Im Blockschaltdiagramm in Abbildung \ref{Blockdiagramm Steuerung} wird die Hardware der Steuerung aufgezeigt. Die genauen Funktionen des Mikrocontroller sind im Prototyping Kapitel \nameref{Blockdiagramm: Schnittstellen zwischen den Komponenten} beschrieben. Im Zentrum steht der Mikrocontroller \gls{tinyk22}.
Er steuert und verarbeitet die Signale der verschiedenen Komponenten wie Motoren, Ultraschallsensor, Liniensensor oder Motortreiber. Die Verbindung zum Raspbberry Pi, der die Bilder auswertet und die Navigation implementiert, wird über \acrfull{uart} aufgebaut. Die Stromüberwachung sorgt dafür, dass keine elektrischen Komponenten durch Überlast beschädigt werden.


\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{img/Blockdiagramm-ET-drawio.drawio-2.png}
    \caption{Blockdiagramm Steuerung}
    \label{Blockdiagramm Steuerung}
\end{figure}


