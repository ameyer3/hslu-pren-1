\subsubsection*{Kürzester Weg finden}
\addcontentsline{toc}{subsubsection}{Kürzester Weg finden}

Damit der Roboter den Graph möglichst schnell traversieren kann, soll berechnet werden, welcher Weg am schnellsten zum Zielknoten führt.

Da es nur 8 Knoten im Graph gibt, wurde von Anfang an vermutet, dass die Geschwindigkeit des Algorithmus vernachlässigt werden kann.

Um zu überprüfen, ob diese These stimmt, wurde ein traditioneller \gls{dijkstra} Algorithmus in Python implementiert. Dabei wurde von einem Knoten der kürzeste Weg zu allen anderen Knoten im vorgegebenen Graphen berechnet. Währenddessen wurde die Zeit für die Berechnungen gestoppt. Um Hardware Einflüsse zu minimieren, wurde dieses Skript auf einem Single-Board Computer, namentlich einem Raspberry Pi 4 (4GB) ausgeführt und mehrmals wiederholt. Dies führt zu folgenden Ausgaben auf Grafik \ref{fig:dijkstra-test-skript-output}:

\begin{figure}[H]
\begin{subfigure}{0.275\textwidth}
\includegraphics[width=0.95\linewidth]{img/graph_with_weighted_edges.png} 
\caption{Gewichteter Graph}
\label{fig:weighted-graph}
\end{subfigure}
\begin{subfigure}{0.720\textwidth}
\begin{footnotesize}
\begin{verbatim}
Shortest distance from E to A is 18 via path: E -> G -> A
Shortest distance from E to B is 22 via path: E -> G -> H -> B
Shortest distance from E to C is 20 via path: E -> D -> C
Shortest distance from E to D is 10 via path: E -> D
Shortest distance from E to E is 0 via path: E
Shortest distance from E to F is 10 via path: E -> F
Shortest distance from E to G is 6 via path: E -> G
Shortest distance from E to H is 12 via path: E -> G -> H
This calculation took about 0.128ms
\end{verbatim}
\end{footnotesize}
\caption{Skript Ausgabe}
\label{fig:dijkstra-test-skript-output}
\end{subfigure}

\caption{Djikstra Algorithmus Test mit Python}
\label{fig:dijkstra-test-output}
\end{figure}

Um den kürzesten Pfad acht Mal zu berechnen, wurden etwa 0,128 ms benötigt, was ausreichend schnell ist. Aufgrund dieses Tests wurde entschieden, einen selbst implementierten, einfachen \gls{dijkstra} Algorithmus zu verwenden. Es ist wichtig, dass der Algorithmus möglichst leichtgewichtig ist, da nur begrenzte Rechenleistung und Speicher zur Verfügung stehen.

Das Skript des Testes wurde in einem Github Gist veröffentlicht und ist unter folgendem Link aufrufbar: \url{https://gist.github.com/dimschlukas/2632116f913b1e10eea9be40e62b2630}

\subsubsection*{Kamera Position}\label{camera-position}
\addcontentsline{toc}{subsubsection}{Kamera Position}

Eine Kamera wird benötigt, um die aktuelle Situation des Graphens zu fotografieren und erkennen zu können, ob Hindernisse platziert sind und wo sich Linien und Knoten befinden. Die Platzierung der Kamera ist ein entscheidender Faktor, um sicherzustellen, dass sie die gewünschten Objekte und Bereiche korrekt erfassen kann. Im Folgenden werden die relevanten Parameter erläutert:

\begin{itemize}
    \item Kamera wird auf einer Höhe von 22.5cm montiert.
    \item Field of View der Kamera beträgt 66\textdegree.
    \item Kamera soll Knoten in einer Nähe bis zu 15cm vor dem Roboter aufzeichnen.
    \item Kamera soll Objekte bis zu 2 Meter Entfernung aufzeichnen.
\end{itemize}

Auf Abbildung \ref{fig:camera-position} ist gezeigt, wie dese Parameter die Position und Neigung der Kamera definieren:

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{assets/informatik-prototyp/camera/camera_position.png}
    \caption{Skizze der Kamera Positionierung}
    \label{fig:camera-position}
\end{figure}

Aus den Parametern kann also berechnet werden, dass die Kamera in einen Winkel von ca. 56\textdegree\ gerichtet werden muss. Auf diese Weise können die Knoten in der Nähe sowie die Pylonen in der Distanz erfasst werden.

Auf Abbildung \ref{fig:camera-position-exact} ist dies dargestellt in einem Graphen mit korrekten Proportionen:

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{assets/informatik-prototyp/camera/camera_position_exact_bigger.png}
    \caption{Graph der Kamera Positionierung}
    \label{fig:camera-position-exact}
\end{figure}

Mit diesem Winkel können nun Knoten bis zu einer horizontalen Distanz von 10cm erkannt werden.

Zusätzlich wurde dies mit dem Raspberry Pi und der Raspberry Pi Kamera getestet. Die Resultate sind auf Abbildung \ref{fig:camera-position-prototype} ersichtlich.

\begin{figure}[H]
\centering
\begin{subfigure}{0.45\textwidth}
\centering
    \includegraphics[height=7cm]{assets/informatik-prototyp/camera/camera_position_prototype_at_home.jpg}
\end{subfigure}
\begin{subfigure}{0.45\textwidth}
\centering
    \includegraphics[height=7cm]{assets/informatik-prototyp/camera/camera_position_prototype.jpg}
\end{subfigure}
\caption{Test der Kamera Position}
\label{fig:camera-position-prototype}
\end{figure}


\subsubsection*{Winkelerkennung}\label{winkelerkennung}
\addcontentsline{toc}{subsubsection}{Winkelerkennung}

Damit der Roboter weiss, in welche Richtung er sich bewegen soll, muss er in der Lage sein, die ausgehenden Linien eines Knotens zu erkennen, damit er diesen folgen kann.

Um die Winkel der ausgehenden Kanten eines Knoten zu detektieren, macht der Roboter vor dem Befahren eines Knotens ein Bild dessen. Dazu ist jedoch zu beachten, dass, wie in Kapitel \nameref{camera-position} beschrieben, unsere Kamera in einem Winkel von 56\textdegree\ montiert wird. So können naheliegende Knoten vor dem Roboter, wie auch weit entfernte Pylonen in einem Bild erkannt werden, ohne dass die Kamera geschwenkt werden muss.
Da dadurch nun die Knoten nicht direkt von oben fotografiert werden, muss an dem aufgenommenen Bild zuerst eine geometrische Transformation durchgeführt werden. Sodass schlussendlich eine Ansicht auf Vogelperspektive auf den Knoten vorliegt. Anschliessend können die einzelnen Winkel ohne Probleme durch maskieren und detektieren von Formen mittels \gls{opencv} berechnet werden.

Nachfolgend wird dies Schritt für Schritt mithilfe der Abbildungen \ref{fig:image-before-node} - \ref{fig:node-with-edge-angles} dargestellt.

\begin{enumerate}
    \item Vor dem Knoten anhalten und Bild aufnehmen
    % \begin{figure}[H]
    %     \centering
    %     \includegraphics[width=0.5\linewidth]{assets/informatik-prototyp/opencv/angle_detection/image_taken_by_pi_camer_before_node.jpg}
    %     \caption{Originale Kamera Aufnahme eines Knotens}
    %     \label{fig:before-node}
    % \end{figure}


    \begin{figure}[H]
        \centering
        \begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=0.95\linewidth]{assets/informatik-prototyp/opencv/angle_detection/image_taken_by_pi_camer_before_node.jpg}
        \caption{Originale Kamera Aufnahme eines Knotens}
        \label{fig:before-node}
        \end{subfigure}
        \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=0.95\linewidth]{assets/informatik-prototyp/opencv/camera_position_before_node.jpg} 
        \caption{Kamera vor Knoten}
        \label{fig:before-node-camera}
        \vspace{5mm}
        \end{subfigure}
        \caption{Aufnahme vor Knoten}
        \label{fig:image-before-node}
        \end{figure}

        
    \item Geometrische Transformation anhand fix definierten Punkten anwenden
        \begin{figure}[H]
        \centering
        \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=0.95\linewidth]{assets/informatik-prototyp/opencv/angle_detection/node_before_transformation_corners.png} 
        \caption{Vier Punkte für Transformation}
        \label{fig:node-before-geometric-transform}
        \end{subfigure}
        \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=0.95\linewidth]{assets/informatik-prototyp/opencv/angle_detection/node_after_transformation.png} 
        \caption{Nach Transformation}
        \label{fig:node-after-geometric-transform}
        \end{subfigure}
        \caption{Geometrische Transformation}
        \label{fig:node-geometric-transform}
        \end{figure}
    \item Knoten maskieren und dessen Mittelpunkt detektieren
        \begin{figure}[H]
        \centering
        \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=0.95\linewidth]{assets/informatik-prototyp/opencv/angle_detection/node_after_transformation_masked.png} 
        \caption{Maskieren}
        \label{fig:node-masked}
        \end{subfigure}
        \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=0.95\linewidth]{assets/informatik-prototyp/opencv/angle_detection/node_detecting_center.png} 
        \caption{Mittelpunkt des Knoten detektieren}
        \label{fig:node-center}
        \end{subfigure}
        \caption{Knoten und dessen Mittelpunkt detektieren}
        \label{fig:detecting-node}
        \end{figure}
    \item Ausgehende Kanten maskieren
        \begin{figure}[H]
        \centering
        \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=0.95\linewidth]{assets/informatik-prototyp/opencv/angle_detection/edge_masked.png} 
        \caption{Nur Kanten maskieren}
        \label{fig:edge-masked}
        \end{subfigure}
        \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=0.95\linewidth]{assets/informatik-prototyp/opencv/angle_detection/edge_contours.png} 
        \caption{Kontur der Kanten zeichnen}
        \label{fig:edge-contours}
        \end{subfigure}
        \caption{Kanten detektieren}
        \label{fig:detecting-edges}
        \end{figure}
    \item Geometrischer Schwerpunkt der Kanten berechnen und Winkel zu Knoten Zentrum berechnen.
        \begin{figure}[H]
        \centering
        \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=0.95\linewidth]{assets/informatik-prototyp/opencv/angle_detection/edge_detect_centers.png} 
        \caption{Mittelpunkte der Kanten berechnen}
        \label{fig:edge-center}
        \end{subfigure}
        \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=0.95\linewidth]{assets/informatik-prototyp/opencv/angle_detection/node_with_edge_angles_annotated.png} 
        \caption{Winkel der Kanten}
        \label{fig:angle-lines}
        \end{subfigure}
        \caption{Winkel der einzelnen Kanten detektieren}
        \label{fig:node-with-edge-angles}
        \end{figure}
\end{enumerate}

\subsubsection*{Graph-, Pylonen- und Barrieren-Erkennung}\label{bilderkennung}
\addcontentsline{toc}{subsubsection}{Graph-, Pylonen- und Barrieren-Erkennung}


In diesem Teil wird vorgestellt, wie das Prototyping fuer die Erkennung von Barrieren und Elementen im Graphen durchgefuehrt wurde. Es wurden mehrere Varianten ausprobiert: SIFT, \gls{orb-gloss}, Farberkennung und \gls{yolo}v11. Am Ende des Kapitels gibt es Tabelle \ref{tab:test-obst-detection}, die die Resultate der Tests mit den einzelnen Methoden auflistet.

Als Basis für die Bilderkennung wurde in der Mensa ein Graph aufgeklebt (Abb. \ref{fig:test-graph}). Es wurden mehrere Bilder gemacht, wobei Pylonen und Barrieren willkürlich auf Knoten und Kanten gestellt wurden und immer wieder verschoben wurden.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{assets/informatik-prototyp/opencv/test_graph.jpg}
    \caption{Aufgeklebter Graph zu Testzwecken}
    \label{fig:test-graph}
\end{figure}

Die Methode, die gewählt wird, muss in der Lage sein folgende Elemente zu erkennen.

\begin{itemize}
    \item Rote Barriere
    \item Weisse Barriere
    \item Pylonen
    \item Knoten
\end{itemize}

\textbf{OpenCV SIFT, FAST, ORB Algorithmen}

\gls{opencv} bietet verschiedene Algorithmen zur Merkmalsdetektion und -beschreibung, die in der Bildverarbeitung und Computer Vision zu Objekterkennung und -tracking eingesetzt werden:

\begin{enumerate}
    \item SIFT (Scale-Invariant Feature Transform)
    
    SIFT ist ein Algorithmus, der robuste und skalierungsinvariante Merkmale in Bildern erkennt. Er findet markante Punkte, sogenannte Keypoints, und berechnet Deskriptoren, die invariant gegenüber Skalierung, Rotation und Beleuchtung sind. Es eignet sich gut für Objektwiedererkennung und Bildregistrierung.

    \item FAST (Features from Accelerated Segment Test)
    
    FAST ist ein sehr schneller Eckendetektor, der besonders für Echtzeitanwendungen geeignet ist. Er überprüft, ob ein Pixel ein Merkmal (Ecke) ist, indem es die Intensitäten der Pixel in einem kreisförmigen Muster um dieses herum vergleicht. Es ist jedoch nicht robust gegenüber Skalierung oder Rotation.

    \item ORB (Oriented FAST and Rotated BRIEF)
    
    ORB kombiniert FAST für die Merkmalsdetektion und BRIEF (Binary Robust Independent Elementary Features) für die Merkmalsbeschreibung. Es erweitert FAST, indem es Orientierung und Rotation berücksichtigt, und bietet eine effiziente, patentfreie Alternative zu SIFT und SURF, mit ähnlicher Genauigkeit und deutlich besserer Performance.
\end{enumerate}

Zusammenfassend:

\begin{itemize}
    \item SIFT ist genau und robust, aber rechenintensiv.
    \item FAST ist schnell, aber weniger robust.
    \item ORB ist ein guter Kompromiss zwischen Geschwindigkeit und Genauigkeit.
\end{itemize}

FAST wurde nicht getestet, da es wichtig ist, dass unser Roboter eine robuste Bilderkennung implementiert hat. Es wurden lediglich SIFT und ORB getestet.

Beide Algorithmen funktionieren sehr gut, um zweidimensionale Objekte zu detektieren, wie nachfolgend an den Stickers auf der Notebook-Rückseite zu erkennen ist (siehe Abbildung \ref{fig:good-sift-example}. Die bunten Linien repräsentieren erkannte Merkmale. Eine Linie führt jeweils von einem gelernten Merkmal (links) zu einem erkannten Merkmal (rechtes Bild).

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{assets/informatik-prototyp/opencv/sift/sift_good_example.png}
    \caption{Gutes Beispiel von SIFT Algorithmus}
    \label{fig:good-sift-example}
\end{figure}

Da unsere Objekte jedoch dreidimensional sind und eine freie Rotation und Position in einem dreidimensionalen Raum haben, ist es für den Algorithmus sehr schwierig Objektmerkmale festzulegen und diese in den verschiedenen Szenarien wieder zu detektieren, wie in Abbildung \ref{fig:sift-orb-in-our-usecase} zu sehen ist:

\begin{figure}[H]
    \centering
\begin{subfigure}{1\textwidth}
    \includegraphics[width=1\linewidth]{assets/informatik-prototyp/opencv/sift/sift_our_usecase_example.png}
    \caption{SIFT Algorithmus}
    \label{fig:bad-sift-example}
\end{subfigure}
\begin{subfigure}{1\textwidth}
    \includegraphics[width=1\linewidth]{assets/informatik-prototyp/opencv/sift/orb_our_usecase_example.png}
    \caption{ORB Algorithmus}
    \label{fig:bad-orb-example}
\end{subfigure}
    \caption{SIFT und ORB in unserem Usecase}
    \label{fig:sift-orb-in-our-usecase}
\end{figure}


Aus diesem Test wird entschieden, dass diese \gls{opencv} Merkmalsdetektionsalgorithmen nicht ausreichen, um Objekte in der 3D Umgebung sauber zu detektieren. Deshalb wird dieser Ansatz nicht mehr weiter verfolgt.

\textbf{Objekterkennung mit Farberkennung}

Grundsätzlich können Objekte auch nur rein durch ihre Farbe detektiert werden. Jedoch sind die Resultate stark von Umgebungsbedingungen abhängig. Orange und rote Objekte können grundsätzlich sehr einfach detektiert werden. Da diese Farben in der Umgebung, wo der Roboter schlussendlich operiert, sehr eindeutig sind. Das Resultat eines Tests mit einer Pylone und einem Hindernis ist auf Abbildung \ref{fig:opencv_hsv_object_detection_good} zu sehen.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{assets/informatik-prototyp/opencv/object_detection_with_hsv/hsv_object_detection.png}
    \caption{Objekterkennung mittels Farberkennung der roten Barriere und orangen Pylone}
    \label{fig:opencv_hsv_object_detection_good}
\end{figure}


Die weisse Barriere und der weisse Knoten macht es jedoch schwieriger. Einerseits haben sie die gleiche Farbe, was das Unterscheiden dieser schwierig macht. Ebenfalls gibt es unter verschiedenen Lichtverhältnissen Spiegelungen, welche sich im Bild auch als weisse Flecken darstellen und somit als Objekte erkennt werden. Die ist ersichtlich in Abbildung \ref{fig:opencv_hsv_object_detection_bad}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{assets/informatik-prototyp/opencv/object_detection_with_hsv/hsv_object_detection_white.png}
    \caption{Objekterkennung mittels Farberkennung der weissen Barriere}
    \label{fig:opencv_hsv_object_detection_bad}
\end{figure}

Mit vielen Feineinstellungen ist es eventuell auch möglich, die weissen Objekte zu detektieren. Jedoch ist es nicht eine sichere Lösung, da die Lichtverhältnisse in dieser Umgebung stark variieren.

\textbf{YOLOv11}

Auf Roboflow\footnote{https://roboflow.com/} wurden die insgesamt 58 erstellten Bilder gelabelt.
Das bedeutet, dass manuell, die zu erkennenden Objekte auf einem Bild ausgewählt und zu bestimmten Klassen hinzugefügt wurden.

Danach wird das Datenset unterteilt in verschiedene Gruppen: Train, Validation, Test. Die Training Daten werden verwendet mit den Markierungen, damit das Model davon lernen kann. Das Validierungsset wird benötigt, um während dem Trainingsprozess zu prüfen, wie gut das Model ist und wie es angepasst werden soll. Das Testset wird schlussendlich verwendet, um zu testen, wie gut das angepasste Model die einzelnen Elemente erkennt. Das Testset ist erforderlich, da die Daten, die zur Bewertung der Modelleistung verwendet werden, dem Model vollständig unbekannt sein müssen. Der Datensplit ist gezeigt auf Abbildung \ref{fig:data-split}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{assets/informatik-prototyp/yolo/dataset-split.png}
    \caption{Datenset Split}
    \label{fig:data-split}
\end{figure}

Als erstes wurde versucht ein Model zu trainieren, das alle Graphenelemente (Knoten und Kanten) und alle Hindernisse erkennt mit den Klassen auf Abbildung \ref{fig:labeling-with-lines} gezeigt:

\begin{figure}[H]
\centering
\begin{subfigure}{0.55\textwidth}
\centering
\includegraphics[width=0.95\linewidth]{assets/informatik-prototyp/yolo/labeled-image-lines.png} 
\caption{Bild mit Elementen + Linie markiert}
\label{fig:labeled-image-lines}
\end{subfigure}
\begin{subfigure}{0.4\textwidth}
\centering
\includegraphics[width=0.95\linewidth]{assets/informatik-prototyp/yolo/labeled-classes-lines.png} 
\caption{Markierte Klassen mit Linie}
\label{fig:line-classes-lines}
\end{subfigure}

\caption{Roboflow labeled Bild mit Linien}
\label{fig:labeling-with-lines}
\end{figure}


Mihilfe eines Jupyter Notebooks, welches von Roboflow zur Verfügung gestellt wird, wurde ein \gls{yolo} Model mit 10 \gls{epoch}s trainiert. Danach wurden Bilder aus dem Testset genutzt, in welchem das Model versucht die einzelnen Elemente zu erkennen. Zusätzlich wurde eine \gls{confusion-matrix} erstellt, die zeigt, welche Elemente als was erkannt wurden, diese ist zu sehen auf Abbildung \ref{fig:recognition-with-lines}.

\begin{figure}[H]
\centering
\begin{subfigure}{0.4\textwidth}
\centering
\includegraphics[width=0.95\linewidth]{assets/informatik-prototyp/yolo/line-recognitions.png} 
\caption{Erkanntes Bild mit Linien}
\label{fig:image-recognition-with-lines}
\end{subfigure}
\begin{subfigure}{0.59\textwidth}
\centering
\includegraphics[width=0.95\linewidth]{assets/informatik-prototyp/yolo/conf-matrix-lines.png} 
\caption{Confusion Matrix mit Linien}
\label{fig:conf-matrix-lines}
\end{subfigure}

\caption{Bilderkennung inklusive Linien}
\label{fig:recognition-with-lines}
\end{figure}

Aus diesem Experiment war klar, sowohl aus dem Bild mit den ``erkannten'' Linien, als auch von der \gls{confusion-matrix}, dass Linien nicht korrekt erkannt werden können.
Aus der \gls{confusion-matrix} kann gelesen werden, dass von allen erkannten Linien in 4 Bildern, 30 Linien erkannt wurden an Stellen, an denen sich gar nichts befindet (``background'').

Als nächstes wurde das gleiche noch einmal durchgeführt. Dieses Mal wurde die Line-Klasse entfernt und es wurden nur noch Knoten, Pylonen und Barrieren markiert. Die Klassen bei diesem Test sind erkenntlich auf Abbildung \ref{fig:labeled-with-lines}.

\begin{figure}[H]
\centering
\begin{subfigure}{0.55\textwidth}
\centering
\includegraphics[width=0.95\linewidth]{assets/informatik-prototyp/yolo/labeled-image.png} 
\caption{Bild mit Elementen markiert}
\label{fig:labeled-image}
\end{subfigure}
\begin{subfigure}{0.4\textwidth}
\centering
\includegraphics[width=0.95\linewidth]{assets/informatik-prototyp/yolo/labeled-classes.png} 
\caption{Markierte Klassen}
\label{fig:line-classes}
\end{subfigure}

\caption{Roboflow labeled Bild mit Linien}
\label{fig:labeled-with-lines}
\end{figure}

Die Resultate der Bilderkennung des selben Jupyter Notebooks mit den reduzierten Klassen sind erkenntlich auf Abbildung \ref{fig:conf-matrix-yolo} und die \gls{confusion-matrix} ist abgebildet auf Abbildung \ref{fig:conf-matrix-yolo}.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth -30mm]{assets/informatik-prototyp/yolo/recognized-images.jpeg}
\caption{YOLOv11 Bilderkennung}
\label{fig:img-recognition-yolo}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth -20mm]{assets/informatik-prototyp/yolo/conf-matrix.png}
\caption{YOLOv11 Confusion Matrix Knoten, Barrieren und Pylonen}
\label{fig:conf-matrix-yolo}
\end{figure}

Aus dieser Matrix kann gelesen werden, dass alle Pylonen und alle Barrieren richtig erkannt werden. Einmal wird eine Pylone erkannt, obwohl es keine gibt. Die meisten vorausgesagten Knoten sind wirklich Knoten, jedoch werden viele Knoten nicht erkannt. Dabei muss jedoch bedacht werden, dass es einfacher ist, für das Model Knoten zu erkennen, die nahe und dadurch deutlicher sind und dass das Trainingsdatenset sehr klein ist. Trotzdem besteht ein sehr wahrscheinliches Risiko, dass ein Knoten nicht erkannt werden könnte.

Das gleiche Experiment wurde noch einmal durchgeführt mit nur Pylonen und Barrieren, die markierten Klassen sind auf Abbildung \ref{fig:labeling-with-cone-obst} gezeigt.

\begin{figure}[H]
\centering
\begin{subfigure}{0.55\textwidth}
\centering
\includegraphics[width=0.95\linewidth]{assets/informatik-prototyp/yolo/label-cone-obstacle.png} 
\caption{Bild mit Hindernissen markiert}
\label{fig:labeled-image-obst}
\end{subfigure}
\begin{subfigure}{0.4\textwidth}
\centering
\includegraphics[width=0.95\linewidth]{assets/informatik-prototyp/yolo/classes-cone-obstacle.png} 
\caption{Markierte Klassen}
\label{fig:cone-obst-classes}
\end{subfigure}
\caption{Roboflow labeled Bild nur mit Hindernissen}
\label{fig:labeling-with-cone-obst}
\end{figure}


Die Resultate für die Pylonen und die Barrieren ist dasselbe, wie im vorherigen Experiment, jedoch fällt die Unsicherheit der Knoten weg. Das Problem ist es nun, dass gedeutet werden würde, dass der erkannte Pylon sich auf dem Nachbarsknoten befindet, was nicht der Fall ist. Der vorherige Knoten musst also erkannt werden können, damit dieser nicht fälschlicherweise als blockiert erkannt wird. Diese Situation ist auf Abbildung \ref{fig:recognition-with-cone-obst} gezeigt.

\begin{figure}[H]
\centering
\begin{subfigure}{0.49\textwidth}
\centering
\includegraphics[width=0.95\linewidth]{assets/informatik-prototyp/yolo/cone-obst-recognition.png} 
\caption{Erkanntes Bild mit Hindernissen}
\label{fig:image-recognition-with-cone-obst}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
\centering
\includegraphics[width=0.95\linewidth]{assets/informatik-prototyp/yolo/cone-obst-conf-matrix.png} 
\caption{Confusion Matrix mit Hindernissen}
\label{fig:conf-matrix-cone-obst}
\end{subfigure}
\caption{Bilderkennung mit Hindernissen}
\label{fig:recognition-with-cone-obst}
\end{figure}



\textbf{Fazit}

In folgender Tabelle \ref{tab:test-target-node} sind die Testresultate der einzelnen Methoden aufgelistet.


\begin{table}[H]
\centering
\small
\begin{tabularx}{\textwidth}{|c|X|X|X|l|}
        \hline
        \textbf{Algorithmus} & \textbf{Kurzbeschreibung} & \textbf{Kriterium zur Erfüllung} & \textbf{Testergebnis} & \textbf{Bewertung} \\
        \hline
        SIFT & Barriere erlernen und erkennen & Barriere wird erkannt & Barriere wird nicht erkannt & Test nicht erfüllt \\ \hline
        ORB & Barriere erlernen und erkennen & Barriere wird erkannt & Barriere wird nicht erkannt & Test nicht erfüllt \\ \hline
        OpenCV Farberkennung & rote Barriere erkennen & Barriere wird erkannt & Barriere wird erkannt & Test erfüllt \\ \hline
        OpenCV Farberkennung & orange Pylone erkennen & Pylone wird erkannt & Pylone wird erkannt & Test erfüllt \\ \hline
        OpenCV Farberkennung & weisser Knoten erkennen & Knoten wird erkannt & Falsche Knoten werden erkannt, Knoten werden nicht erkannt & Test nicht erfüllt \\ \hline
        OpenCV Farberkennung & weisse Barriere erkennen & Barrier wird erkannt & Falsche Barrieren werden erkannt & Test nicht erfüllt \\ \hline
        YOLOv11 & Barriere erkennen & Barrier wird erkannt & Barrieren werden erkannt & Test erfüllt \\ \hline
        YOLOv11 & Pylone erkennen & Pylone wird erkannt & Pylone werden erkannt & Test erfüllt \\ \hline
        YOLOv11 & Knoten erkennen & Knoten wird erkannt & Knoten werden meist erkannt & Test meist erfüllt \\ \hline
        YOLOv11 & Kanten erkennen & Kanten wird erkannt & Kanten werden nicht erkannt & Test nicht erfüllt \\ \hline

\end{tabularx}
    \caption{Testergebnisse Hindernisse erkennen}
\label{tab:test-obst-detection}
\end{table}

Zur Erkennung der Hindernisse und des Graphens funktionierte \gls{yolo}v11 am besten. Dabei wird versucht die Hindernisse und die Graphenknoten zu erkennen. Die Unsicherheit der Knotenerkennung ist jedoch ein grosses Risiko. Deshalb wird eine Strategie verfolgt, die mit den ausgehenden Linien vom Knoten arbeitet und nur die Nachbarsknoten erkennen will.


\subsubsection*{Zielknoten erkennen}
\addcontentsline{toc}{subsubsection}{Zielknoten erkennen}

Der Roboter muss in der Lage sein, die Buchstaben auf den Zielknoten zu erkennen, um sicherzustellen, dass er sich wirklich im Ziel befindet. Dazu wurden drei verschiedene Varianten evaluiert: \gls{tesseract} Algorithmus, \gls{easyocr} und \acrfull{orb}.

Die Tests wurden bei allen drei Varianten mit den folgenden drei Bildern (Abbildung \ref{fig:muster-zielknoten}) durchgeführt. Diese sind unveränderte Aussschnitte aus dem zur Verfügung gestellten Muster der Zielknoten. Die Bilder wurden nicht pre-processed, da sie bereits in perfekter Qualität sind.

Die Methode, die gewählt wird muss in der Lage sein folgendes zu tun:

\begin{itemize}
    \item Buchstaben A erkennen
    \item Buchstaben B erkennen
    \item Buchstaben C erkennen
    \item Mit Rotation der Buchstaben umgehen
\end{itemize}

\begin{figure}[H]
\centering
\begin{subfigure}{0.35\textwidth}
\centering
\includegraphics[height=30mm]{assets/informatik-prototyp/opencv/target_node_detection/a.png} 
\caption{Muster A}
\label{fig:image-a}
\end{subfigure}
\begin{subfigure}{0.2\textwidth}
\centering
\includegraphics[height=30mm]{assets/informatik-prototyp/opencv/target_node_detection/b.png} 
\caption{Muster B}
\label{fig:image-b}
\end{subfigure}
\begin{subfigure}{0.33\textwidth}
\centering
\includegraphics[height=30mm]{assets/informatik-prototyp/opencv/target_node_detection/c.png} 
\caption{Muster C}
\label{fig:image-c}
\end{subfigure}

\caption{Muster der Zielknoten}
\label{fig:muster-zielknoten}
\end{figure}

\textbf{Tesseract OCR}

\gls{tesseract} ist ein bekannter Algorithmus um Text zu lesen. Es gibt einen Modus, der sich darauf spezialisiert nur einzelne Buchstaben anstatt Textblöcke zu lesen. Dieser wurde bei dem Prototyping verwendet mit der Option \verb|--psm 10|. Ausserdem wurde festgelegt, dass nur die Buchstaben A, B und C erkannt werden können. Dies wurde wie folgt mit der Python Tesseract Bibliothek\footnote{\url{https://pypi.org/project/pytesseract/}} implementiert. 

\begin{verbatim}
text = pytesseract.image_to_string(image, config='--psm 10 -c
tessedit_char_whitelist=ABC')
\end{verbatim}

Dabei konnte \gls{tesseract} ohne Probleme die Buchstaben A und C lesen, jedoch konnte der Buchstabe B nicht gelesen werden, die Resultate sind gezeigt in Abbildung \ref{fig:zielknoten-tesseract}:

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{assets/informatik-prototyp/opencv/target_node_detection/tesseract.png} 
\caption{Zielknotenerkennung mit Tesseract}
\label{fig:zielknoten-tesseract}
\end{figure}

\textbf{EasyOCR}

\gls{easyocr} wird zur Extrahierung von Text verwendet. Da \acrshort{ai} verwendet wird dazu, benötigt dieser sowohl mehr Rechenpower, als auch mehr Zeit, die Bilder zu verarbeiten. Dieser Algorithmus ist Teil von der \gls{opencv} Bibliothek. EasyOCR hat die Buchstaben wie auf Abbildung \ref{fig:zielknoten-easyocr} gezeigt erkannt:

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{assets/informatik-prototyp/opencv/target_node_detection/easyocr.png} 
\caption{Zielknotenerkennung mit EasyOCR}
\label{fig:zielknoten-easyocr}
\end{figure}

Bei EasyOCR sind zwei Probleme aufgetreten. Zum einen ist die Ausführung des Algorithmus bereits auf einem Laptop recht langsam und zum anderen wird der Buchstabe C als Klammer-Auf \verb|(| erkannt.

\textbf{\acrfull{orb}}

\gls{orb-gloss} lernt die Merkmale von Objekten und erkennt auf einem Bild, welches Objekt gefunden wurde. Alle gefundenen Matches wurden in diesem Experiment nach Wahrscheinlichkeit sortiert. Mit diesem Verfahren konnten alle Knoten erkannt werden. Die Musterknoten wurden um 35 Grad gedreht, um zu prüfen, ob ORB auch Buchstaben erkennt, wenn sie nicht gleich rotiert sind wie auf dem Muster Bild. Die Detektierung der Buchstaben ist auf Abbildung \ref{fig:orb-zielknoten} gezeigt.

\begin{figure}[H]
\centering
\begin{subfigure}{0.3\textwidth}
\centering
\includegraphics[width=0.95\linewidth]{assets/informatik-prototyp/opencv/target_node_detection/orb-a.png} 
\caption{ORB A}
\label{fig:orb-a}
\end{subfigure}
\begin{subfigure}{0.3\textwidth}
\centering
\includegraphics[width=0.95\linewidth]{assets/informatik-prototyp/opencv/target_node_detection/orb-b.png} 
\caption{ORB B}
\label{fig:orb-b}
\end{subfigure}
\begin{subfigure}{0.3\textwidth}
\centering
\includegraphics[width=0.95\linewidth]{assets/informatik-prototyp/opencv/target_node_detection/orb-c.png} 
\caption{ORB C}
\label{fig:orb-c}
\end{subfigure}

\caption{Zielknotenerkennung mit ORB}
\label{fig:orb-zielknoten}
\end{figure}

\textbf{Fazit}

Die Resultate aus den Tests der vorherigen Kapiteln sind in folgender Tabelle \ref{tab:test-target-node} zusammengefasst.

\begin{table}[H]
\centering
\small
\begin{tabularx}{\textwidth}{|c|X|X|X|l|}
        \hline
        \textbf{Algorithmus} & \textbf{Kurzbeschreibung} & \textbf{Kriterium zur Erfüllung} & \textbf{Testergebnis} & \textbf{Bewertung} \\
        \hline
        Tesseract OCR & Buchstabe A erkennen & A wird erkannt & A erkannt & Test erfüllt \\ \hline
        Tesseract OCR & Buchstabe B erkennen & B wird erkannt & nichts erkannt & Test nicht erfüllt \\ \hline
        Tesseract OCR & Buchstabe C erkennen & C wird erkannt &  C erkannt & Test erfüllt \\ \hline 
        EasyOCR & Buchstabe A erkennen & A wird erkannt & A erkannt & Test erfüllt \\ \hline
        EasyOCR & Buchstabe B erkennen & B wird erkannt & B erkannt &  Test erfüllt \\ \hline
        EasyOCR & Buchstabe C erkennen & C wird erkannt & ( ``Klammer auf'' erkannt & Test nicht erfüllt \\ \hline
        ORB & Buchstabe A erkennen & A wird erkannt & A erkannt & Test erfüllt \\ \hline
        ORB & Buchstabe B erkennen & B wird erkannt & B erkannt & Test erfüllt \\ \hline
        ORB & Buchstabe C erkennen & C wird erkannt & C erkannt & Test erfüllt \\ \hline
        ORB & Buchstabe A mit Rotation erkennen & A wird erkannt & A erkannt & Test erfüllt \\ \hline
        ORB & Buchstabe B mit Rotation erkennen & B wird erkannt & B erkannt & Test erfüllt \\ \hline
        ORB & Buchstabe C mit Rotation erkennen & C wird erkannt & C erkannt & Test erfüllt \\ \hline

\end{tabularx}
    \caption{Testergebnisse Zielknoten erkennen}
\label{tab:test-target-node}
\end{table}

Zur Erkennung der Buchstaben wird der \acrshort{orb} Algorithmus verwendet werden. Dieser hatte keine Probleme damit die Buchstaben klar zu erkennen und es werden sicher nur A, B oder C erkannt, da der Algorithmus nur diese Buchstaben kennt. Auf diese Weise kann verhindert werden, dass zum Beispiel `C' als ein Klammer-auf \verb|(| gelesen wird. Ebenfalls ist keine Rotation des Bildes des Zielknotens nötig, da \gls{orb} die Buchstaben auch rotiert erkennt.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% PROTOTYPE PRODUKTE ALLG %%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{Physische Prototypen}\label{prototyp-products}
\addcontentsline{toc}{subsection}{Physische Prototypen}

In diesem Kapitel werden die einzelnen Prototypen vorgestellt, die das Bauen des Roboters in \acrshort{pren2} parallelisieren werden. Die Prototypen können entweder einzeln verwendet werden oder schnell zusammengefügt werden und in Kombination miteinander genutzt werden.

\subsubsection*{Navigation}
\addcontentsline{toc}{subsubsection}{Navigation}


Damit die Navigation implementiert werden kann, braucht es eine Vorrichtung, welche die Kamera stabil in der definierten Position befestigt. Ebenfalls soll es die Möglichkeit geben, bei Bedarf einen Ultraschallsensor daran zu befestigen, um Hindernisse zu erkennen, wenn der Roboter sich direkt davor befindet. Dies ist eine Risikominderung, falls die Kamera Hindernisse nicht richtig erkennt. Dazu wurde eine Turmvorrichtung gebaut, an der die Kamera befestigt werden kann. Sie befindet sich auf einem Sockel, der verwendet werden kann, um bei Bedarf Sensoren, die nach vorne zeigen, anzubringen.

\begin{figure}[H]
\centering
\begin{subfigure}{0.4\textwidth}
\centering
\includegraphics[width=0.95\linewidth]{assets/informatik-prototyp/camer_tower_1.png} 
\end{subfigure}
\begin{subfigure}{0.4\textwidth}
\centering
\includegraphics[width=0.95\linewidth]{assets/informatik-prototyp/camer_tower_2.png} 
\end{subfigure}
\caption{Prototyp Kamera Turm}
\label{fig:prototype-camera-tower}
\end{figure}

\subsubsection*{Fortbewegung}
\addcontentsline{toc}{subsubsection}{Fortbewegung}

Damit die Fortbewegung inklusive Linienerkennung implementiert werden kann, benötigt es vor allem die Räder und die Motoren. Diese wurden zusammen mit einem Stützrad an einem Holzbrett befestigt, das ebenfalls verwendet werden kann, um die zusätzlichen Sensoren und Rechner anzubringen, die für die Steuerung benötigt werden. Am Ende von \acrshort{pren1} wurde bereits ein Ultraschallsensor an der Vorderseite befestigt.

\begin{figure}[H]
\centering
\begin{subfigure}{0.32\textwidth}
\centering
\includegraphics[width=0.95\linewidth]{assets/prototyp-fahrwerk/fahrwerk-motoren.jpeg} 
\end{subfigure}
\begin{subfigure}{0.32\textwidth}
\centering
\includegraphics[width=0.95\linewidth]{assets/prototyp-fahrwerk/fahrwerk-oben.jpeg} 
\end{subfigure}
\begin{subfigure}{0.32\textwidth}
\centering
\includegraphics[width=0.95\linewidth]{assets/prototyp-fahrwerk/fahrwerk-unten.jpeg} 
\end{subfigure}
\caption{Fahrwerk Prototyp}
\label{fig:prototype-fahrwerk}
\end{figure}

\subsubsection*{Greifer}
\addcontentsline{toc}{subsubsection}{Greifer}


Um ein Hindernis anzuheben, wurde ein Greifer, welcher über ein Gelenkgestänge funktioniert, ausgelegt. Um dieses Konzept zu Testen wurden zwei Prototypen gebaut (Kapitel \nameref{subsubsection:gripper-prototype-1} und Kapitel \nameref{subsubsection:gripper-prototype-2}). Der zweite Prototyp wird, wie der finale Greifer später auch, mit einem Servomotor betrieben. 

\begin{figure}[H]
\centering
\begin{subfigure}{0.32\textwidth}
\centering
\includegraphics[width=0.95\linewidth]{assets/greifer-prototyp/Bild_Greifer_2_offen.jpeg} 
\end{subfigure}
\begin{subfigure}{0.32\textwidth}
\centering
\includegraphics[width=0.95\linewidth]{assets/greifer-prototyp/Bild_Greifer_2_klemmen.jpeg} 
\end{subfigure}
\begin{subfigure}{0.32\textwidth}
\centering
\includegraphics[width=0.95\linewidth]{assets/greifer-prototyp/Bild_greifer_2_anheben.jpeg} 
\end{subfigure}
\caption{Greifer Prototyp II }
\label{fig:prototype-greifer}
\end{figure}

